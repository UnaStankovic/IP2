% !TEX encoding = UTF-8 Unicode

\documentclass[a4paper]{article}

\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage[T2A]{fontenc} % enable Cyrillic fonts
\usepackage[utf8]{inputenc} % make weird characters work
\usepackage{graphicx}
\usepackage{mwe}    % loads »blindtext« and »graphicx«
\usepackage{subfig}
\usepackage[english,serbian]{babel}
%\usepackage[english,serbianc]{babel} %ukljuciti babel sa ovim opcijama, umesto gornjim, ukoliko se koristi cirilica
\usepackage{listings}
\usepackage{minted}
\usepackage[unicode]{hyperref}
\hypersetup{colorlinks,citecolor=green,filecolor=green,linkcolor=blue,urlcolor=blue}

%\newtheorem{primer}{Пример}[section] %ćirilični primer
\newtheorem{primer}{Primer}[section]

\begin{document}

\title{Skripta za vežbe\\ \small{Skripta u okviru kursa\\Istraživanje podataka 2\\ Matematički fakultet}}

\date{24.~sept 2018.}
\maketitle


\tableofcontents

\newpage

\section{Uvod}
\label{sec:uvod}

S obzirom na činjenicu da se ovaj kurs direktno nastavlja na kurs Istraživanje podataka 1, kao i da postoji uslovljenost nekim predmetima, podrazumeva se poznavanje osnovnih koncepata. Iako će neki od njih biti ponovno objašnjeni u daljem tekstu, fokus ovog teksta je na novim konceptima. 

\begin{primer}
Prvi primer služi kao podsetnik na određene mogućnosti koje nam pruža \texttt{pandas} biblioteka.  
Najpre, importujemo biblioteku, potom uz pomoć \texttt{read\_csv} vršimo učitavanje $.csv$ fajla koji sadrži podatke koje želimo da analiziramo i na kraju vršimo ispise različitih vrednosti. Kao što vidimo \texttt{pandas} biblioteka nam kroz \texttt{DataFrame} nudi mnogobrojne načine pregledanja podataka, koje možemo koristiti na različite načine. U nastavku su navedeni osnovni. 
\end{primer}
\inputminted{python}{Codes/1/pandas-example.py}
\begin{lstlisting}
  sepal_length  sepal_width  petal_length  petal_width    species
0        	  5.1          3.5           1.4          0.2     setosa
1             4.9          3.0           1.4          0.2     setosa
2             4.7          3.2           1.3          0.2     setosa
3             4.6          3.1           1.5          0.2     setosa
4             5.0          3.6           1.4          0.2     setosa
5             5.4          3.9           1.7          0.4     setosa
...
[150 rows x 5 columns]
______________________________________________________________________
   sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa

______________________________________________________________________
sepal_length    float64
sepal_width     float64
petal_length    float64
petal_width     float64
species          object
dtype: object
Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',
       'species'],
      dtype='object')
RangeIndex(start=0, stop=150, step=1)
[[5.1 3.5 1.4 0.2 'setosa']
 [4.9 3.0 1.4 0.2 'setosa']
 [4.7 3.2 1.3 0.2 'setosa']
________________________________________________________________________
     sepal_width  petal_length
0            3.5           1.4
1            3.0           1.4
2            3.2           1.3
3            3.1           1.5
4            3.6           1.4
...
[150 rows x 2 columns]
\end{lstlisting}


\subsection{Istraživanje Veba - Web Mining}
\label{sec:veb}
Kada posmatramo distribuirani sistem informacija, vidimo da su dokumenti i objekti najčešće povezani kako bi se omogućio interaktivni pristup. Glavni primer distribuiranog sistema je Veb, gde korisnici, u potrazi za informacijama, $"$putuju$"$ od objekta do objekta uz pomoć hiperlinkova i \texttt{URL} adresa. Veb konstantno raste i svaki dan se dodaje po nekoliko miliona stranica. Sa ovakvim, neprekidnim, rastom dostupnih stranica i podataka uviđa se da dobijanje informacija iz takvih izvora postaje sve teže i teže. Kao jedan od glavnih problema pri analizi Veb stranica javlja se nedostatak strukture. Pod $"$rudarenjem (istraživanjem) Veba$"$\footnote{engl. web mining} podrazumevamo korišćenje tehnika za $"$rudarenje (istraživanje) podataka$"$\footnote{engl. data mining} kako bi se automatski otkrile i izvlačile informacije sa Veb dokumenata i servisa. Web mining možemo podeliti na četiri glavne celine:
\begin{enumerate}
\item pronalaženje resursa - prikupljanje informacija preko na primer Web stranica kao što su stranice sa vestima i slično,
\item odabir informacija i pretprocesiranje - različite vrste transformacija nad podacima: uklanjanje stop reči, dobijanje željene reprezentacije i drugo,
\item generalizacija - proces automatskog otkrivanja uobičajenih uzoraka (engl. \em{pattern}) korišćenjem različitih opštih tehnika mašinskog učenja, istraživanja podataka i raznih veb-orijentisanih metoda i  
\item analiza - vrši se validacija i/ili interpretacija istraženih uzoraka\footnote{engl.  mined patterns}. 
\end{enumerate}
Jedna od mogućih podela istraživanja Veba odnosi se na to koji deo Veba neko želi da analizira. Postoje tri vida istraživanja Veba:
\begin{enumerate}
\item Istraživanje sadržaja Veba (engl. \em{Web content mining}) - koristi sadržaj Veb stranice kako bi prikupio podatke: tekst, slike, video ili bilo koji drugi sadržaj,
\item Istraživanje strukture Veba (engl. \em{Web structure mining}) - fokusira se na strukturu veza veb stranice,
\item Istraživanje upotrebe Weba (engl. \em{Web usage mining}) - kao podatke koristi podatke prikupljene od interakcija korisnika pri korišćenju interneta.
\end{enumerate}

Najčešća upotreba istraživanja sadržaja Veba je u procesu pretrage. Na primer, crawler-i se koriste od strane pretraživača da izvuku sadržaj Veb strane kako bi se odmah dobili traženi rezultati. Isto tako, oni se mogu koristiti tako da im fokus bude na određenoj temi ili oblasti interesovanja, umesto da zahtevaju sve informacije koje se mogu dostići.\\\\
Kako bi se kreirao fokusirani crawler, klasifikator se obično trenira na skupu podataka odabranih od strane korisnika, kako bi crawler znao koji tip sadržaja traži. Potom on identifikuje stranice od interesa kako na njih nalazi i prati dalje sve linkove sa te stranice. Ako linkovi sa neke stranice od interesa vode ka nekim drugim stranicama koje su klasifikovane kao stranice koje nisu od interesa onda se linkovi na tim stranama dalje ne posmatraju. Istraživanje sadržaja se može najlakše direktno videti u procesu pretrage. Svi veći pretraživači danas koriste strukturu koja podseća na listu. Ta lista je uređena uz pomoć algoritma za rangiranje stranica.\\

U \texttt{Pythonu} postoji modul \texttt{sklearn.feature\_extraction} koji služi za izvlačenje objekata\footnote{engl. features} iz skupova podataka koji sadrže tekst ili slike, u formatu koji je podržan od strane algoritama mašinskog učenja. 

\subsubsection{Izvlačenje teksta}

Analiza teksta je jedna od važnih primena algoritama mašinskog učenja. Međutim, sirovi podaci\footnote{engl. raw data} predstavljaju prepreku utoliko što ih algoritmima ne možemo dati direktno, već nam je potrebna neka njihova numerička reprezentacija uz pomoć vektora fiksirane dužine. Kako bi se ovaj problem rešio \texttt{scikit\-learn} nudi nekoliko načina da se iz tekstualnog sadržaja izvuku numerički podaci, princip koji se koristi je sledeći:
\begin{itemize}
\item dodeljivanje tokena stringova i davanje $id$-ja za svaki od tokena, npr. korišćenjem razmaka ili zareza kao odvajajućih elemenata između dva tokena,
\item brojanje pojavljivanja tokena u svakom od dokumenata,
\item normalizacija i dodeljivanje težina sa smanjenjem bitnosti onih tokena koji se često pojavljuju
\end{itemize}
Korpus dokumenata se, dakle, može predstaviti uz pomoć matrice koja ima jedan red po dokumentu i jednu kolonu po tokenu (reči) koje se javljaju u korpusu.\\\\
\textbf{Vektorizacijom} nazivamo proces pretvaranja kolekcije tekst dokumenata u numeričke vektore objekata\footnote{engl. numerical feature vectors}. Konkretna strategija navedena gore (tokenizacija, brojanje i normalizacija) naziva se \textit{Bag of Words} reprezentacija. Dokumenti su opisani pojavljivanjima reči ignorišući informacije o poziciji reči u dokumentima.\\

\texttt{CountVectorizer} iz \texttt{sklearn.feature\_extraction.text} implementira i tokenizaciju i brojanje pojavljivanja u istoj klasi. Korišćenje $"$stop$"$ reči, poput $"$and$"$, $"$the$"$, itd., za koje se pretpostavlja da ne nose nikakve informacije u kontekstu teksta, mogu biti uklonjene. Nekada, međutim, može se desiti da su slične reči korisne za predikciju, tako da sa korišćenjem $"$stop$"$ reči treba biti oprezan. Jedna od najpoznatijih $"$stop$"$ lista reči je $"$english$"$.
U velikim korpusima teksta, neke reči će se ponavljati veoma često, samim tim takve reči ni ne nose puno značajnih informacija o stvarnom sadržaju dokumenta. Ako bismo dali podatke o broju reči klasifikatoru direktno, one reči koje se veoma često ponavljaju bi poremetile pojavljivanja onih reči koje se ređe ponavljuju, a samim tim su nam i zanimljivije. Kako bismo ponovo izmerili i dodelili težine objektima, u vrednostima u pokretnom zarezu koji je pogodan za klasifikator, veoma često koristićemo tf-idf transformaciju. \\
$Tf$ označava učestalost pojavljivanja nekog termina\footnote{engl. term-frequency}, dok tf-idf se odnosi na učestalost pojavljivanja nekog termina puta inverzna učestalost dokumenta\footnote{engl. inverse document frequency}.
\begin{equation}
tfidf(t,d) = tf(t,d) \times idf(t)
\end{equation}

Korišćenjem osnovnih podešavanja \texttt{TfidfTransformer}-a : TfidfTransformer(norm='l2', use\_idf=True, smooth\_idf=True, sublinear\_tf=False), učestalost termina, broj puta koji se on pojavljuje u dokumentu, množi se sa $idf$ komponentom, koja se računa kao:
\begin{equation}
idf(t) = \log(\frac{1 + n_d}{1 + df(d,t)}) +1
\end{equation}
gde je $n_d$ ukupni broj dokumenata, $df(d,t)$ je broj dokumenata koji sadrže termin $t$. Rezultujući tf-idf vektori se potom normalizuju Euklidskom normom. Glavni cilj korišćenja tf-idf umesto sirovih učestalosti pojavljivanja tokena u dokumentu je skaliranje uticaja tokena koji se jako često pojavljuju u korpusu i time su empirijski manje informativni.\\
Da bismo matricu učestalosti pojavljivanja\footnote{engl. count matrix} transformisali u normalizovanu tf ili tf-idf reprezentaciju koristimo\\ 
\texttt{sklearn.feature\_extraction.text.TfidfTransformer}.\\
Kako ne bismo svaki put radili \texttt{CountVectorizer}, pa potom \texttt{TfidfTransformer}, postoji ugrađena bibliotečka podrška, u vidu:\\
\texttt{sklearn.feature\_extraction.text.TfidfVectorizer}\\
koji konvertuje kolekciju sirovih dokumenata u matricu TF-IDF objekata.\\

\begin{figure}[t]
\includegraphics[width=8cm]{Pictures/slika1.png}
\centering
\caption{Prikaz odnosa učestalosti pojavljivanja reči i njihovog značaja.}
\end{figure}

Pošto nam je cilj da se bavimo obradom prirodnih jezika, za tako nešto možemo iskoristiti \texttt{nltk}\footnote{engl. The Natural Language Toolkit}. On predstavlja skup biblioteka i programa za simboličko i statističko procesiranje prirodnih jezika, konkretno engleskog jezika. Predstavlja vodeću platformu za izradu \texttt{Python} programa za rad sa prirodnim jezikom, pa tako sadrži skup biblioteka za procesiranje teksta pri klasifikaciji, tokenizaciju, izvlačenje korena reči\footnote{engl. stemming}, tagovanje, parsiranje itd. Stemeri\footnote{engl. Stemmers} uklanjaju morfološke prefikse, sufikse i infikse iz reči, kako bi ostala samo srž. Postoji više različitih vrsta stemera, od kojih će prvi biti predstavljen \texttt{SnowballStemmer}, koji podržava naredne jezike: danski, holandski, engleski, finski, francuski, nemački, mađarski, italijanski, norverški, portugalski, rumunski, ruski, španski i švedski.\\\\
Kada hoćemo da vršimo klasifikaciju, to možemo učiniti pomoću više različitih klasifikatora koje nam nudi \texttt{sklearn}, kao što su:
\begin{itemize}
\item KNeighborsClassifier - ima višestruke primene u finansijama, zdravstvu, političkim naukama, detekciji rukopisa, prepoznavanju slika i videa, itd.. Koristi se i za klasifikaciju i za regresiju i bazira se na pristupu sličnosti objekata. $K$ predstavlja broj najbližih suseda. Ako je $K=1$, onda za algoritam kažemo da je samo algoritam najbližeg suseda (engl. \em{nearest neighbor algorithm}).
\item SGDClassifier - Stohastički gradijentni spust (engl. \em{Stochastic Gradient Descent-SGD}) je jednostavan, ali veoma efikasan pristup, čije se prednosti ogledaju u efikasnosti, jednostavnoj implementaciji, a mane su mu što zahteva određeni broj parametara (npr. regularizacioni parametar, broj iteracija) i osetljiv je na skaliranje objekata. 
\item MultinomialNB - Naivni Bajesof klasifikator (engl. \em{Naive Bayes classifier}) for multinomial models) je pogodan za klasifikaciju diskretnih objekata. 
\end{itemize}

\begin{figure}[t]
\includegraphics[width=8cm]{Pictures/slika2.png}
\centering
\caption{Prikaz K-najbližih suseda.}
\end{figure}
\newpage
\subsubsection{Predikcija kategorije teksta}
\begin{primer}
Naredni primer vrši predikciju kategorije teksta. Najpre se učitavaju podaci iz fajla \texttt{articles.csv}, u kome su podaci smešteni u obliku $kategorija, tekst$. Kolone koje čine fajl smeštamo u dve promenljive redom, $y$ i $texts$. Nakon toga, vrši se inicijalizacija vektorizatora za kreiranje TF-IDF matrice. Potom, primenjuje se neki od 3 klasifikatora:
\begin{itemize}
\item KNeighborsClassifier
\item SGDClassifier
\item MultinomialClassifier
\end{itemize}
Nakon fitovanja klasifikacionog modela, računarju se preciznosti na trening i test skupu, a potom se učitava test model i vrši se predikcija kategorije teksta. 
\end{primer}
\inputminted{python}{Codes/1/classifier.py}
Rezultat izvršavanja ovog programa je oblika:
\begin{lstlisting}
Train acc: 0.9871547848426461
Test acc: 0.968562874251497
['business']
['business', 'entertainment', 'politics', 'sport', 'tech']
[[0.38475301 0.09423106 0.28803524 0.11219421 0.12078647]]
\end{lstlisting}
\newpage
\subsubsection{Latentna semantička analiza}
\textbf{Latentna semantička analiza}\footnote{engl. LSA - latent semantic analysis-LSA} predstavlja način za particionisanje teksta korišćenjem statističkih modela upotrebe reči koji je sličan dekompoziciji sopstvenih vektora i analizi. Umesto da se samo fokusiramo na površne objekte kao što je učestalost reči, ovaj pristup obezbeđuje kvanitativnu meru semantičkih sličnosti među dokumentima baziranom na kontekstu reči. Dve glavne mane su sinonimija i polisemija. Sinonimija se odnosi na različite reči istog ili sličnog značenja. Sa druge strane, polisemija se odnosi na reči koje imaju više različitih značenja. \texttt{LSA} pokušava da razreši ovaj tip problema, bez pomoći rečnika i sredstava na obradu prirodnih jezika, već korišćenjem matematičkih uzoraka koji postoje unutar podataka. To se postiže smanjenjem dimenzije koja se koristi da se dokument predstavi korišćenjem matematičke matrične operacije koja se naziva singularna dekompozicija\footnote{engl. SVD - Singular Value Decomposition}.\\

\textbf{Singularna dekompozicija} razbija bilo koju matricu $A$ na $A=U*S*V'$. Ako bismo bliže pogledali matricu $S$, videli bismo da je $S$ matrica forme, takve, da se sastoji od matrice $D$, koja na dijagonali sadrži sve $\sigma$, koje predstavljaju singularne vrednosti. Broj ovih singularnih vrednosti nam govori o rangu matrice $A$. Možemo pronaći aproksimaciju redukovanog ranga\footnote{engl. \em{truncated SVD} ili \em{LSA}} za $A$ tako što postavimo sve, osim prvih $k$ najvećih singularnih vrednosti na nulu, a potom koristimo samo prvih $k$ kolona matrica $U$ i $V$. Ovaj postupak vršimo kako bismo izvršili \textbf{redukciju dimenzionalnosti}. Suprotno PCA\footnote{engl. PCA - Principal Component Analysis}, ovaj estimator ne centrira podatke pre izračunavanja singularne dekompozicije, što mu omogućava da radi efikasno sa retkim matricama\footnote{Retke matrice su one matrice koje sadrže veliki broj nula.} (dostupnim kroz modul \texttt{scipy.sparse}). Konkretno, truncated SVD radi nad matricama sa brojevima termova\footnote{engl. term count} ili nad $tfidf$ matricama. U kontekstu ovih drugih, postupak je poznat kao latentna semantička analiza.\\
\begin{primer}
Ovaj primer vrši klasifikaciju novinskih članaka. Najpre se učitavaju podaci iz fajla \texttt{articles.csv}, u kome su podaci smešteni u obliku $kategorija, tekst$. Kolone koje čine fajl smeštamo u dve promenljive redom, $y$ i $texts$. Nakon toga, vrši se inicijalizacija vektorizatora za kreiranje TF-IDF matrice. Potom se vrši LSA.

\inputminted{python}{Codes/2/0-lsa.py}
Rezultat izvršavanja ovog programa predstavlja skup reči sa najvećim težinama po komponentama oblika:
\begin{lstlisting}
Component 182

('scottish', 0.09168672123398844)
('play', 0.08953887985792805)
('age', 0.08291233924728324)
('ethnic', 0.08288040224246893)
('murder', 0.08153061624591648)
('edward', 0.07253791078410225)
('develop', 0.07211813150850585)
('foster', 0.07116597034292958)
('minor', 0.07032073815220793)
('sentenc', 0.06973781343683193)
\end{lstlisting}
\end{primer}

\subsubsection{Web Scraping} 
Da bismo prikupili neke informacije sa veb stranica, a želimo to da uradimo na jednostavan način, potreban nam je program koji bi automatski izdvojio informacije od interesa i izvršio to izdvajanje za nas. Ovakav program naziva se skrejper\footnote{engl. scraper}. On obilazi određenu vezu (ili, još češće, više veza na stranicama) i izdvaja željene informacije. Takav jedan program bio bi nam, na primer, potreban kako bismo popunili neki $.csv$ fajl u koji bismo mogli da smestimo neke novinske članke. Da bismo tako nešto postigli, moramo za svaku $.html$ stranicu koju posetimo:
\begin{enumerate}
\item da izdvojimo relevantne informacije i
\item da nađemo linkove ka ostalim stranama.
\end{enumerate}
Kako bismo to uradili moramo koristiti regularne izraze i \texttt{re} biblioteku koja nam omogućava rad sa njima. Da bismo uopšte mogli da $"$dovučemo$"$ stranice moramo koristiti \texttt{urllib.request}. 
Urllib je paket koji sadrži nekoliko modula za rad sa URL-ovima:
\begin{itemize}
\item \texttt{urllib.request} - za otvaranje i čitanje URL-ova,
\item \texttt{urllib.error} - sadrži izuzetke koji se mogu javiti korišćenjem urllib.request,
\item \texttt{urllib.parse} - za parsiranje URL-ova i 
\item \texttt{urllib.robotparser} - za parsiranje \texttt{robots.txt} fajlova.
\end{itemize}
U slučaju da je neki sajt na ćirilici, ne želimo da ga menjamo, ali u slučaju da je na latinici, želimo da, uz pomoć \texttt{translit} modula \texttt{transliterate}, prevedemo tekst iz latinice u ćirilicu. \texttt{Transliterate} je dvosmerni modul za prevođenje u Pythonu. Prevodi niske prema pravilima koja su specifična u svakom jezičnom paketu (izvorni skript <-> ciljni skript). Nudi mogućnost rada sa narednim jezicima: jermenski, bugarski, gruzijski, grčki, makedonski, mongolski, ruski, srpski, ukrajinski.  

\begin{primer}
Naredni primer upravo vrši opisani postupak.


\inputminted{python}{Codes/2/1-scraper.py}

\begin{lstlisting}
/rss/
/scc/pretraga
/scc/rubrika/1/Svet
/scc/rubrika/2/Politika
/scc/rubrika/3/Drustvo
/scc/rubrika/4/Pogledi
/scc/rubrika/5/Hronika
/scc/rubrika/6/Ekonomija
/scc/rubrika/7/Kultura
/scc/rubrika/9/Srbija
/scc/rubrika/10/Beograd
/scc/rubrika/8/Sport
/scc/rubrika/29/Region
/scc/sarena-strana
/scc/rubrika/396/Magazin
/scc/rubrika/34/Moj-zivot-u-inostranstvu
/scc/satires/index
/scc/rubrika/1060/TV-revija
/scc/rubrika/1073/Tema-nedelje
/scc/clanak/414061/Slucaj-revizora-Sretenovica
clanak:
/scc/autor/913/Jovana-Rabrenovic
/scc/clanak/414064/Vlada-Crne-Gore-Srusicemo-crkvu-na-Rumiji
clanak:
/scc/clanak/414066/SAD-nisane-Rusiju-a-gadaju-Kinu
clanak:
/scc/autor/854/Jelena-Stevanovic
/scc/clanak/414096/Putin-Nas-odgovor-na-americke-rakete-u-Evropi-bice-brz-i-efikasan
clanak:
/scc/clanak/414059/U-rukama-ucenika-umesto-olovaka-cigarete
clanak:
/scc/clanak/414086/Sport/Ubedljiva-pobeda-Liverpula-protiv-Zvezde
clanak:
/scc/rubrika/49/Fudbal
/scc/clanak/414069/Oluja-krivac-za-rakete-ispaljene-iz-Gaze-na-Izrael
clanak:
/scc/clanak/414062/Zima-ce-biti-blaga-sa-malo-padavina
clanak:
/scc/clanak/414057/Humorom-protiv-straha
clanak:
/scc/clanak/414071/Pojacana-prodaja-novih-automobila
clanak:
/scc/clanak/414097/Sport/Kosarka/Dabl-dabl-Bjelice-u-pobedi-Kingsa-nad-Memfisom
clanak:
/scc/rubrika/50/Kosarka
/scc/clanak/414093/Americki-avion-predvodio-dronove-u-napadu-na-rusku-bazu-u-Siriji
\end{lstlisting}
\end{primer}


\begin{primer}
Nakon što smo prikupili neke podatke sa stranica, želimo sada i na neki način da te podatke analiziramo. U ovom primeru želimo da vršimo pretragu stranica na osnovu traženih pojmova, koje unosimo u vidu \texttt{query\_string}. 
\inputminted{python}{Codes/2/2-analisys.py}

Izlaz iz programa predstavlja prvih 5 najbitnijih stranica za zadate pojmove. 
\begin{lstlisting}
http://www.politika.rs/scc/clanak/414025/Spanske-firme-sele-centrale-iz-regiona-u-Beograd
http://www.politika.rs/scc/clanak/413970/Linta-Da-srpske-zrtve-u-Hrvatskoj-i-BiH-docekaju-pravdu
http://www.politika.rs/scc/clanak/413321/Skot-Kosovo-je-suverena-zemlja
http://www.politika.rs/scc/clanak/413988/Bramerc-o-predstojecem-izvestaju-Mladicu-radikalima
http://www.politika.rs/scc/clanak/413777/Kosovo-cas-anatomije
\end{lstlisting}
\end{primer}
\newpage
\subsubsection{PageRank algoritam}
Naredni primer odnosi se na tzv. PageRank algoritam, odnosno, algoritam za rangiranje stranica. Ovaj algoritam je originalno objavljen od strane ljudi koji su učestvovali u kreiranju Google-a, Sergeja Brina i Lari Pejdža i smatra se odgovornim za njegov rani uspeh. Funkcioniše tako što stranice posmatra kao čvorove u grafu, a veze između stranica kao grane i potom obezbeđuje globalno rangiranje stranica na webu (čvorova u grafu). Za pretraživače obezbeđuje rangiranje stranica nezavisno od upita. Glavna pretpostvka ovog algoritma je da je svaka veza od stranice $a$ ka stranici $b$ glas od stranice $a$ za stranicu $b$. Bitno je naglasiti da nije svaki glas jednake težine. Težine dodeljuje PageRank algoritam na osnovu početnog čvora. Iterativna formula za PageRank je sledeća:
\begin{equation}
PR_{i+1}(P_i) = \sum\nolimits_{P_j}\frac{PR_i(P_j)}{C(P_j)}
\end{equation}
$PR_{i+1}$ predstavlja PageRank neke stranice $P_i$ u narednoj iteraciji i računa se kao suma količnika PageRank-ova stranica koje pokazuju na tu stranicu u prethodnim iteracijama i broja $"$odlaznih$"$ stranica. Da bismo bolje razumeli šta nam zapravo navedena formula govori posmatrajmo sliku \ref{fig:pr1}. 
\begin{figure}[t]
\includegraphics[width=6cm]{Pictures/pr1.png}
\centering
\label{fig:pr1}
\caption{Prikaz grafa. A, B, C i D predstavljaju stranice.}
\end{figure}


Pošto je PageRank iterativan algoritam, u prvoj iteraciji rangove svih stranica postavljamo na $\frac{1}{ukupanbrojstranica}$, u primeru sa slike to bi bilo $\frac{1}{4}$. 
\begin{itemize}
\item $PR_0(A) = \frac{1}{4}$
\item $PR_0(B) = \frac{1}{4}$
\item $PR_0(C) = \frac{1}{4}$
\item $PR_0(D) = \frac{1}{4}$
\end{itemize}
Potom u prvoj iteraciji (prikazanoj na slikama \ref{fig:PR1}), posmatramo nultu i ukupan broj strana na koje pokazuje svaki od čvorova koji pokazuju na stranicu koju analiziramo.

\begin{figure}[!ht]
	\subfloat[Stranica C ima vezu (pokazuje) na stranicu A. \label{subfig-1:pr2}]
	{%
		\includegraphics[width=0.44\textwidth]{Pictures/pr2.png}
     }
	\hfill
	\subfloat[Stranice A i C imaju vezu (pokazuju) na stranicu B. \label{subfig-1:pr3}]
	{%
		\includegraphics[width=0.40\textwidth]{Pictures/pr3.png}
     }
	\hfill
	\subfloat[Stranice A i D imaju vezu (pokazuju) na stranicu C. \label{subfig-1:pr4}]
	{	%
		\includegraphics[width=0.42\textwidth]{Pictures/pr4.png}
    }
	\hfill
	\subfloat[Stranice B i C imaju vezu (pokazuju) na stranicu D. \label{subfig-1:pr5}]
	{%
		\includegraphics[width=0.41\textwidth]{Pictures/pr5.png}
    }
	\hfill
    \caption{PageRank prikaz prve iteracije.}
    \label{fig:PR1}
\end{figure}

Na primeru, ako posmatramo čvor $A$, na njega pokazuje samo čvor $C$, koji pokazuje na čvorove $A$, $B$ i $D$. To znači da iz prethodne iteracije imamo $\frac{1}{4}$, a potom sve to delimo sa $3$ (broj strana na koje se pokazuje sa strane $C$). Analogno, važi za ostale čvorove. Dakle, dobijamo sledeće formule:
\begin{itemize}
\item $PR_1(A) = \frac{\frac{1}{4}}{3} = \frac{1}{12}$
\item $PR_1(B) = \frac{\frac{1}{4}}{2} + \frac{\frac{1}{4}}{3} = \frac{2.5}{12}$
\item $PR_1(C) = \frac{\frac{1}{4}}{2} + \frac{\frac{1}{4}}{1} =\frac{4.5}{12}$
\item $PR_1(D) = \frac{\frac{1}{4}}{1} + \frac{\frac{1}{4}}{3} = \frac{4}{12}$
\end{itemize}

U narednoj iteraciji, ponovo vršimo isti postupak:
\begin{itemize}
\item $PR_2(A) = \frac{\frac{4.5}{12}}{3} = \frac{1.5}{12}$
\item $PR_2(B) = \frac{\frac{1}{12}}{2} + \frac{\frac{4.5}{12}}{3} = \frac{2}{12}$
\item $PR_2(C) = \frac{\frac{1}{12}}{2} + \frac{\frac{4}{12}}{1} = \frac{4.5}{12}$
\item $PR_2(D) = \frac{\frac{2.5}{12}}{1} + \frac{\frac{4.5}{12}}{3} = \frac{4}{12}$
\end{itemize}
Na kraju, dobijamo da je page rank:
\begin{itemize}
\item $PR(A) = 1$
\item $PR(B) = 2$
\item $PR(C) = 4$
\item $PR(D) = 3$
\end{itemize}
Primetimo da se u svakoj iteraciji sume svih rangova sumiraju na 1. Što je viši broj u imeniocu razlomka, to je ta stranica važnija. Tako se uviđa da je na primeru stranica $C$ najvažnija. Iza nje sledi stranica $D$, ako bismo pogledali sliku, možda nam na prvi pogled ne bi bilo odmah jasno zašto je stranica $D$ druga najvažnija. Ovaj događaj je posledica toga što se važnost stranice određuje u odnosu na broj drugih važnih stranica koje ukazuju na nju. Tako je $D$, važnija od stranica $A$ i $B$.\\
\subparagraph{Matrični pristup}
Osim iterativnog pristupa za PageRank algoritam možemo koristiti i matrični pristup. Redom su $A$, $B$, $C$ i $D$ vrste i kolone matrice. Po kolonama, redom, smeštamo moguće verovatnoće izbora određenog čvora dostupnog iz tekućeg. Na primer, iz čvora $A$ možemo doći do čvorova $B$ i $C$, ali ne možemo do $D$, tako da u koloni $A$ na pozicijama $B$ i $C$ upisujemo po jednu polovinu. Iz $B$, možemo doći samo do $D$, itd. Uočimo, da je ponovo suma po kolonama jednaka jedinici. Matrica izgleda na sledeći način:
\[
\begin{bmatrix}
    0           & 0 & \frac{1}{3} & 0\\
    \frac{1}{2} & 0 & \frac{1}{3} & 0\\
    \frac{1}{2} & 0 & 0           & 1\\
    0           & 1 & \frac{1}{3} & 0
\end{bmatrix}
\]

Ovakvu matricu nazivamo $"$stohastička$"$ matrica, obeležavamo je sa $H$ i množimo sa nekim vektorom $v$, dimenzija $1 \times n$, gde $n$ predstavlja broj čvorova u grafu, odnosno, broj stranica. Na osnovu navedenog možemo izvesti formulu:
\begin{equation}
P_{i+1} = Hv
\end{equation}
Vektor $v$ je vrednost PageRank-a u prethodnim iteracijama, pa prethodnu formulu možemo predstaviti kao:
\begin{equation}
P_{i+1} = HP_i
\end{equation}
Naredna iteracija vektora $v$ je:
\begin{equation}
v_2 = Hv
\end{equation}
\begin{equation}
v_3 = Hv_2 = H(Hv) = H^2v
\end{equation}
Ako bismo nastavili da iteriramo, svaki put dodavajući $H$, posle nekog vremena došli bismo do stanja ekvilibrijuma, koje označava da ne treba dalje da iteriramo, jer smo našli vrednosti koje smo tražili. U vektoru $v$ nam se nalaze rangovi stranica. Ovaj metod naziva se \textbf{$"$power method$"$}.\\
Potencijalni problemi koji se mogu javiti prilikom primene PageRank algoritma:
\begin{itemize}
\item $"$viseći$"$ čvorovi - algoritam neće raditi u situaciji kada imamo čvorove na koje niko ne pokazuje,
\item ako graf sadrži komponente koje nisu povezane.
\end{itemize} 
\begin{primer} 
Ovaj primer upravo implementira PageRank algoritam. Izlaz ovog algoritma neće biti predstavljen.
\inputminted{python}{Codes/2/3-pagerank.py}
\end{primer}


\addcontentsline{toc}{section}{Literatura}
\appendix
\bibliography{seminarski} 
\bibliographystyle{plain}

\end{document}
